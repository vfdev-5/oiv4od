{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert annotations to MS Coco format with more meta fields\n",
    "\n",
    "Convert datasets: train 500k, validation and test\n",
    "\n",
    "We added into annotations:\n",
    "- 'IsOccluded'\n",
    "- 'IsTruncated'\n",
    "- 'IsDepiction'\n",
    "- 'IsInside'\n",
    "\n",
    "Apply prefiltering: \n",
    "- images with any dimension larger 2000 pixels are ignored\n",
    "- do not write annotations without bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as_mscoco\r\n",
      "challenge2018\r\n",
      "class-descriptions-boxable.csv\r\n",
      "lost+found\r\n",
      "output-OpenImagesObjectDetections\r\n",
      "test\r\n",
      "test-annotations-bbox.csv\r\n",
      "test-annotations-human-imagelabels-boxable.csv\r\n",
      "train\r\n",
      "train-annotations-bbox.csv\r\n",
      "train-annotations-human-imagelabels-boxable.csv\r\n",
      "train-images-boxable-with-rotation.csv\r\n",
      "validation\r\n",
      "validation-annotations-bbox.csv\r\n",
      "validation-annotations-human-imagelabels-boxable.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505563\n",
      "125436\n",
      "41620\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/train | wc -l\n",
    "!ls ../input/test | wc -l\n",
    "!ls ../input/validation | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from pathlib import Path\n",
    "except ImportError:\n",
    "    from pathlib2 import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = Path(\".\").resolve().parent / \"input\" / \"train\"\n",
    "TRAIN_ANNOTATIONS_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"train-annotations-bbox.csv\"\n",
    "TRAIN_CONFIDENCE_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"train-annotations-human-imagelabels-boxable.csv\"\n",
    "TRAIN_IMGINFO_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"train-images-with-rotation.csv\"\n",
    "VALIDATION_IMAGES_PATH = Path(\".\").resolve().parent / \"input\" / \"validation\"\n",
    "VALIDATION_ANNOTATIONS_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"validation-annotations-bbox.csv\"\n",
    "VALIDATION_CONFIDENCE_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"validation-annotations-human-imagelabels-boxable.csv\"\n",
    "VALIDATION_IMGINFO_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"validation-images-with-rotation.csv\"\n",
    "LABELS_DESCRIPTION_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"class-descriptions-boxable.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGES_PATH = Path(\".\").resolve().parent / \"input\" / \"test\"\n",
    "TEST_ANNOTATIONS_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"test-annotations-bbox.csv\"\n",
    "TEST_CONFIDENCE_CSV_PATH = Path(\".\").resolve().parent / \"input\" / \"test-annotations-human-imagelabels-boxable.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_description = pd.read_csv(LABELS_DESCRIPTION_CSV_PATH, header=None)\n",
    "labels = labels_description[1].values.tolist()\n",
    "\n",
    "coco_categories = []\n",
    "for i, label in enumerate(labels):\n",
    "    coco_categories.append({\n",
    "        'id': i,\n",
    "        'name': label,\n",
    "        'supercategory': label\n",
    "    })\n",
    "    \n",
    "categories = {}\n",
    "for d in coco_categories:\n",
    "    categories[d['name']] = d['id']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 600)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(list(categories.values())), max(list(categories.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyxy_cols = ['XMin', 'YMin', 'XMax', 'YMax']\n",
    "meta_cols = ['IsOccluded', 'IsTruncated', 'IsGroupOf', 'IsDepiction', 'IsInside']\n",
    "ignore_is_crowd = True\n",
    "\n",
    "\n",
    "def to_pixels(xs, scale, vmin, vmax):\n",
    "    return np.clip(xs * scale, vmin, vmax)\n",
    "\n",
    "def get_bboxes_labels_meta(canvas_size, image_id, annotations):\n",
    "    bboxes = annotations.loc[image_id, xyxy_cols].values\n",
    "    labels = annotations.loc[image_id, 'LabelName']\n",
    "    meta = annotations.loc[image_id, meta_cols].values\n",
    "    \n",
    "    if bboxes.ndim == 1:\n",
    "        bboxes = bboxes[None, :]\n",
    "        meta = meta[None, :]\n",
    "    \n",
    "    if isinstance(labels, str):        \n",
    "        labels = np.array([labels, ])\n",
    "        \n",
    "    # BBox format should be (x, y, w, h)\n",
    "    bboxes[:, 0] = to_pixels(bboxes[:, 0], canvas_size[0], 0, canvas_size[0] - 1)\n",
    "    bboxes[:, 1] = to_pixels(bboxes[:, 1], canvas_size[1], 0, canvas_size[1] - 1)\n",
    "    \n",
    "    bboxes[:, 2] = to_pixels(bboxes[:, 2], canvas_size[0], 0, canvas_size[0] - 1)\n",
    "    bboxes[:, 2] -= bboxes[:, 0] - 1\n",
    "    \n",
    "    bboxes[:, 3] = to_pixels(bboxes[:, 3], canvas_size[1], 0, canvas_size[1] - 1)\n",
    "    bboxes[:, 3] -= bboxes[:, 1] - 1\n",
    "    return bboxes, labels, meta\n",
    "\n",
    "\n",
    "def compute_area(bbox):\n",
    "    return bbox[2] * bbox[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def _task(images_path, image_id):\n",
    "    p = images_path / \"{}.jpg\".format(image_id)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "\n",
    "    img = Image.open(images_path / \"{}.jpg\".format(image_id))\n",
    "    \n",
    "    if max(img.size) > 2000 or min(img.size) < 100:\n",
    "        return None\n",
    "\n",
    "    image_info = {\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": \"{}.jpg\".format(image_id),\n",
    "            \"width\": img.size[0],\n",
    "            \"height\": img.size[1],\n",
    "    }    \n",
    "    return image_info\n",
    "\n",
    "\n",
    "\n",
    "def create_annotations_json(images_path, annotations, coco_categories, output_mode, \n",
    "                            image_ids=None, ignore_is_crowd=True):\n",
    "    \n",
    "    coco_images = []\n",
    "    coco_annotations = []\n",
    "    \n",
    "    if image_ids is None:\n",
    "        image_ids = annotations.index.unique()        \n",
    "            \n",
    "            \n",
    "    image_infos = list([None] * len(image_ids))\n",
    "    with Parallel(n_jobs=16) as parallel:\n",
    "        bs = 48\n",
    "        for i in tqdm.tqdm(range(0, len(image_ids), bs)):\n",
    "            batch_image_ids = image_ids[i:i + bs]\n",
    "            image_infos[i:i + bs] = parallel(delayed(_task)(images_path, image_id) for image_id in batch_image_ids)\n",
    "    image_infos = [i for i in image_infos if i is not None]\n",
    "                    \n",
    "    for image_info in tqdm.tqdm(image_infos):\n",
    "        \n",
    "        image_id = image_info['id']\n",
    "        img_size = (image_info['width'], image_info['height'])\n",
    "                \n",
    "        bboxes, labels, meta = get_bboxes_labels_meta(img_size, image_id, annotations)\n",
    "\n",
    "        if len(bboxes) == 0:\n",
    "            print(\"No bboxes for image_id '{}'\".format(image_id))\n",
    "            continue\n",
    "        \n",
    "        num_added_annotations = 0\n",
    "        \n",
    "        for i, (bbox, label, m) in enumerate(zip(bboxes, labels, meta)):\n",
    "            m = [int(v) for v in m]\n",
    "            bbox = [int(v) for v in bbox.tolist()]\n",
    "            label = categories[label]\n",
    "            \n",
    "            if bbox[2] < 1 or bbox[3] < 1:\n",
    "                continue\n",
    "\n",
    "            if label < 0 or label > 600:\n",
    "                continue\n",
    "            \n",
    "            annotation_id = hash(image_id + \"_{}\".format(i))\n",
    "            annotation_info = {\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": label,\n",
    "                \"IsOccluded\": m[0],\n",
    "                \"IsTruncated\": m[1],\n",
    "                \"iscrowd\": m[2] if not ignore_is_crowd else 0,\n",
    "                \"IsDepiction\": m[3],\n",
    "                \"IsInside\": m[4],            \n",
    "                \"area\": int(compute_area(bbox)),\n",
    "                \"bbox\": bbox,\n",
    "                \"segmentation\": [],\n",
    "            } \n",
    "            coco_annotations.append(annotation_info)  \n",
    "            num_added_annotations += 1\n",
    "\n",
    "        if num_added_annotations > 0:\n",
    "            coco_images.append(image_info)\n",
    "\n",
    "    output_coco_annotations = {\n",
    "        \"categories\": coco_categories,\n",
    "        \"images\": coco_images,\n",
    "        \"annotations\": coco_annotations\n",
    "    }\n",
    "    \n",
    "    output_folder = Path(\".\").resolve().parent / \"input\" / \"as_mscoco\" / \"annotations\" \n",
    "    if not output_folder.exists():\n",
    "        output_folder.mkdir(parents=True)\n",
    "    \n",
    "    with open((output_folder / \"{}.json\".format(output_mode)).as_posix(), 'w') as h:\n",
    "        json.dump(output_coco_annotations, h)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = TRAIN_IMAGES_PATH\n",
    "annotations_path = TRAIN_ANNOTATIONS_CSV_PATH\n",
    "output_mode = \"train\"\n",
    "\n",
    "annotations = pd.read_csv(annotations_path, index_col=\"ImageID\")\n",
    "annotations['LabelName'] = annotations['LabelName'].map(labels_description.set_index(0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3457/36314 [01:04<10:10, 53.84it/s]"
     ]
    }
   ],
   "source": [
    "create_annotations_json(images_path, annotations, coco_categories, output_mode, \n",
    "                        ignore_is_crowd=ignore_is_crowd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = VALIDATION_IMAGES_PATH\n",
    "annotations_path = VALIDATION_ANNOTATIONS_CSV_PATH\n",
    "output_mode = \"val\"\n",
    "\n",
    "annotations = pd.read_csv(annotations_path, index_col=\"ImageID\")\n",
    "annotations['LabelName'] = annotations['LabelName'].map(labels_description.set_index(0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_annotations_json(images_path, annotations, coco_categories, output_mode, \n",
    "                        ignore_is_crowd=ignore_is_crowd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = TEST_IMAGES_PATH\n",
    "annotations_path = TEST_ANNOTATIONS_CSV_PATH\n",
    "output_mode = \"test\"\n",
    "\n",
    "annotations = pd.read_csv(annotations_path, index_col=\"ImageID\")\n",
    "annotations['LabelName'] = annotations['LabelName'].map(labels_description.set_index(0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_annotations_json(images_path, annotations, coco_categories, output_mode, \n",
    "                        ignore_is_crowd=ignore_is_crowd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train dataset to check overfitting\n",
    "\n",
    "- 10 images from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = TEST_IMAGES_PATH\n",
    "annotations_path = TEST_ANNOTATIONS_CSV_PATH\n",
    "output_mode = \"train_overfit\"\n",
    "\n",
    "annotations = pd.read_csv(annotations_path, index_col=\"ImageID\")\n",
    "annotations['LabelName'] = annotations['LabelName'].map(labels_description.set_index(0)[1])\n",
    "image_ids = annotations.index.unique()\n",
    "image_ids = image_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 87.62it/s]\n"
     ]
    }
   ],
   "source": [
    "create_annotations_json(images_path, annotations, coco_categories, output_mode, \n",
    "                        ignore_is_crowd=ignore_is_crowd, image_ids=image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create symlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "drwxr-xr-x 6 root root 4096 Aug  4 18:20 .\n",
      "drwxrwxr-x 7 1000 1000 4096 Aug  4 13:13 ..\n",
      "drwxr-xr-x 2 root root 4096 Aug  4 14:11 annotations\n",
      "drwxr-xr-x 3 root root 4096 Jul 12 23:31 annotations_with_iscrowd\n",
      "drwxr-xr-x 2 root root 4096 Jul  6 23:13 .ipynb_checkpoints\n",
      "lrwxrwxrwx 1 root root   70 Jul  6 23:10 test -> /home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/test\n",
      "lrwxrwxrwx 1 root root   71 Aug  4 18:20 train -> /home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/train\n",
      "drwxr-xr-x 2 root root 4096 Jul 13 06:48 train_overfit\n",
      "lrwxrwxrwx 1 root root   76 Jul  6 23:03 val -> /home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/validation\n"
     ]
    }
   ],
   "source": [
    "!ls -all ../input/as_mscoco/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = \"train\"\n",
    "images_path = TRAIN_IMAGES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_images_folder = Path(\".\").resolve().parent / \"input\" / \"as_mscoco\" / output_mode\n",
    "if not output_images_folder.exists():\n",
    "    output_images_folder.symlink_to(images_path, target_is_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = \"val\"\n",
    "images_path = VALIDATION_IMAGES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_images_folder = Path(\".\").resolve().parent / \"input\" / \"as_mscoco\" / output_mode\n",
    "if not output_images_folder.exists():\n",
    "    output_images_folder.symlink_to(images_path, target_is_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = \"test\"\n",
    "images_path = TEST_IMAGES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_images_folder = Path(\".\").resolve().parent / \"input\" / \"as_mscoco\" / output_mode\n",
    "if not output_images_folder.exists():\n",
    "    output_images_folder.symlink_to(images_path, target_is_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = \"train_overfit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_images_folder = Path(\".\").resolve().parent / \"input\" / \"as_mscoco\" / output_mode\n",
    "\n",
    "if not output_images_folder.exists():\n",
    "    output_images_folder.mkdir()\n",
    "\n",
    "for image_id in image_ids:\n",
    "    !ln -s {images_path.as_posix()}/{image_id}.jpg {output_images_folder}/{image_id}.jpg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000026e7ee790996.jpg  0002ab0af02e4a77.jpg  00045d609ca3f4eb.jpg\n",
      "000062a39995e348.jpg  0002cc8afaf1b611.jpg  00068d5450f0358b.jpg\n",
      "0000c64e1253d68f.jpg  0003d84e0165d630.jpg\n",
      "000132c20b84269b.jpg  000411001ff7dd4f.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls {output_images_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 4 root root 4096 Aug  2 06:31 .\n",
      "drwxrwxrwx 8 1000 1000 4096 Jul 28 18:47 ..\n",
      "drwxr-xr-x 2 root root 4096 Aug  2 00:27 annotations\n",
      "lrwxrwxrwx 1 root root   31 Jul 28 14:58 test -> /home/project/oiv4od/input/test\n",
      "lrwxrwxrwx 1 root root   32 Aug  2 06:31 train -> /home/project/oiv4od/input/train\n",
      "drwxr-xr-x 2 root root 4096 Jul 28 11:22 train_overfit\n",
      "lrwxrwxrwx 1 root root   37 Jul 28 14:58 val -> /home/project/oiv4od/input/validation\n"
     ]
    }
   ],
   "source": [
    "!ls -all ../input/as_mscoco/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools import coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=6.48s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "output_folder = Path(\".\").resolve().parent / \"input\" / \"as_mscoco\" / \"annotations\" \n",
    "\n",
    "coco = coco.COCO((output_folder / \"test.json\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = coco.getImgIds()\n",
    "image_ids.sort()\n",
    "roidb = coco.loadImgs(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 215976)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(roidb), len(roidb) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_name': '5840d582ce4fbe93.jpg',\n",
       "  'height': 683,\n",
       "  'id': '5840d582ce4fbe93',\n",
       "  'width': 1024}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco.loadImgs(['5840d582ce4fbe93', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check complete datasets on errors:\n",
    "- no annotations, \n",
    "- annotation has zero or negative size\n",
    "- annotation is out of bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_folder = Path(\".\").resolve().parent / \"input\" / \"as_mscoco\" / \"annotations\" \n",
    "annotations_file = output_folder / \"train.json\"\n",
    "\n",
    "with open(annotations_file.as_posix(), 'r') as h:\n",
    "    annotations = json.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_images = {}\n",
    "for im in annotations['images']:\n",
    "    annotations_images[im['id']] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "invalid_bboxes = []\n",
    "\n",
    "for a in annotations['annotations']:\n",
    "    bbox = a['bbox']\n",
    "    img_info = annotations_images[a['image_id']]\n",
    "    w = img_info['width']\n",
    "    h = img_info['height']\n",
    "    assert 0 <= bbox[0] <= w, \"Problem with {}, {}\".format(a, img_info)\n",
    "    assert 0 <= bbox[1] <= h, \"Problem with {}, {}\".format(a, img_info)\n",
    "    assert 0 <= bbox[0] + bbox[2] <= w, \"Problem with {}, {}\".format(a, img_info)\n",
    "    assert 0 <= bbox[1] + bbox[3] <= h, \"Problem with {}, {}\".format(a, img_info)\n",
    "    if bbox[2] < 1 or bbox[3] < 1:\n",
    "        invalid_bboxes.append((a['image_id'], a['id'], bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3707911, 474169)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['annotations']), len(annotations['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = VALIDATION_IMAGES_PATH\n",
    "annotations_path = VALIDATION_ANNOTATIONS_CSV_PATH\n",
    "\n",
    "annotations = pd.read_csv(annotations_path, index_col=\"ImageID\")\n",
    "annotations['LabelName'] = annotations['LabelName'].map(labels_description.set_index(0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = \"a2f7ab86fb274aa0\"\n",
    "\n",
    "img = Image.open(images_path / \"{}.jpg\".format(image_id))\n",
    "\n",
    "if max(img.size) > 2000 or min(img.size) < 100:\n",
    "    raise RuntimeError(\"\")\n",
    "\n",
    "image_info = {\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": \"{}.jpg\".format(image_id),\n",
    "        \"width\": img.size[0],\n",
    "        \"height\": img.size[1],\n",
    "}    \n",
    "bboxes, labels, meta = get_bboxes_labels_meta(img.size, image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations = []\n",
    "for i, (bbox, label, m) in enumerate(zip(bboxes, labels, meta)):\n",
    "    m = [int(v) for v in m]\n",
    "    bbox = [int(v) for v in bbox.tolist()]\n",
    "    annotation_id = hash(image_id + \"_{}\".format(i))\n",
    "    annotation_info = {\n",
    "        \"id\": annotation_id,\n",
    "        \"image_id\": image_id,\n",
    "        \"category_id\": categories[label],\n",
    "        \"IsOccluded\": m[0],\n",
    "        \"IsTruncated\": m[1],\n",
    "        \"iscrowd\": m[2] if not ignore_is_crowd else 0,\n",
    "        \"IsDepiction\": m[3],\n",
    "        \"IsInside\": m[4],            \n",
    "        \"area\": int(compute_area(bbox)),\n",
    "        \"bbox\": bbox,\n",
    "        \"segmentation\": [],\n",
    "    }\n",
    "    coco_annotations.append(annotation_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 132,\n",
       "  'bbox': [127, 711, 11, 12],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810697,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 180,\n",
       "  'bbox': [131, 461, 10, 18],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810698,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 96,\n",
       "  'bbox': [133, 492, 8, 12],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810699,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 150,\n",
       "  'bbox': [134, 738, 10, 15],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810700,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 20,\n",
       "  'bbox': [300, 416, 4, 5],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810701,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 16,\n",
       "  'bbox': [302, 426, 4, 4],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810702,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 25,\n",
       "  'bbox': [313, 478, 5, 5],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810703,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 24,\n",
       "  'bbox': [314, 487, 4, 6],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810704,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 35,\n",
       "  'bbox': [371, 482, 5, 7],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810689,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 30,\n",
       "  'bbox': [371, 472, 6, 5],\n",
       "  'category_id': 14,\n",
       "  'id': -9011256856298810690,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 16,\n",
       "  'bbox': [382, 412, 4, 4],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638671,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 30,\n",
       "  'bbox': [382, 401, 6, 5],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638670,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 20,\n",
       "  'bbox': [443, 549, 5, 4],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638669,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 0,\n",
       "  'bbox': [445, 588, 0, 5],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638668,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 6,\n",
       "  'bbox': [445, 581, 3, 2],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638667,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 10,\n",
       "  'bbox': [456, 473, 2, 5],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638666,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 20,\n",
       "  'bbox': [456, 481, 5, 4],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638665,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 30,\n",
       "  'bbox': [541, 464, 6, 5],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638664,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 24,\n",
       "  'bbox': [544, 478, 4, 6],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638663,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 20,\n",
       "  'bbox': [566, 377, 5, 4],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957480638662,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 15,\n",
       "  'bbox': [567, 385, 3, 5],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957483638678,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 30,\n",
       "  'bbox': [661, 384, 5, 6],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957483638679,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 30,\n",
       "  'bbox': [665, 399, 5, 6],\n",
       "  'category_id': 14,\n",
       "  'id': 5930169957483638676,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 413660,\n",
       "  'bbox': [129, 169, 559, 740],\n",
       "  'category_id': 147,\n",
       "  'id': 5930169957483638677,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 534303,\n",
       "  'bbox': [73, 147, 693, 771],\n",
       "  'category_id': 176,\n",
       "  'id': 5930169957483638674,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 198,\n",
       "  'bbox': [127, 253, 11, 18],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957483638675,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 180,\n",
       "  'bbox': [136, 213, 10, 18],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957483638672,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 63,\n",
       "  'bbox': [331, 635, 9, 7],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957483638673,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 90,\n",
       "  'bbox': [354, 290, 10, 9],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957483638686,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 84,\n",
       "  'bbox': [382, 552, 7, 12],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957483638687,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 91,\n",
       "  'bbox': [405, 280, 7, 13],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638673,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 52,\n",
       "  'bbox': [441, 650, 4, 13],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638672,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 72,\n",
       "  'bbox': [441, 715, 6, 12],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638675,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 99,\n",
       "  'bbox': [444, 680, 9, 11],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638674,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 130,\n",
       "  'bbox': [499, 780, 13, 10],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638677,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 132,\n",
       "  'bbox': [522, 596, 12, 11],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638676,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 55,\n",
       "  'bbox': [570, 567, 5, 11],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638679,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 66,\n",
       "  'bbox': [579, 726, 6, 11],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638678,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 55,\n",
       "  'bbox': [584, 705, 5, 11],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638681,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 63,\n",
       "  'bbox': [608, 267, 7, 9],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957482638680,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 65,\n",
       "  'bbox': [616, 231, 5, 13],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957477638656,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 144,\n",
       "  'bbox': [660, 836, 12, 12],\n",
       "  'category_id': 252,\n",
       "  'id': 5930169957477638657,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 467347,\n",
       "  'bbox': [69, 165, 629, 743],\n",
       "  'category_id': 291,\n",
       "  'id': 5930169957477638658,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 551232,\n",
       "  'bbox': [68, 134, 696, 792],\n",
       "  'category_id': 297,\n",
       "  'id': 5930169957477638659,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 84972,\n",
       "  'bbox': [10, 592, 291, 292],\n",
       "  'category_id': 365,\n",
       "  'id': 5930169957477638660,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 63104,\n",
       "  'bbox': [25, 374, 272, 232],\n",
       "  'category_id': 365,\n",
       "  'id': 5930169957477638661,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 71383,\n",
       "  'bbox': [31, 92, 247, 289],\n",
       "  'category_id': 365,\n",
       "  'id': 5930169957477638662,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 533708,\n",
       "  'bbox': [81, 147, 686, 778],\n",
       "  'category_id': 432,\n",
       "  'id': 5930169957477638663,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 453492,\n",
       "  'bbox': [78, 165, 612, 741],\n",
       "  'category_id': 501,\n",
       "  'id': 5930169957477638664,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 440,\n",
       "  'bbox': [129, 477, 20, 22],\n",
       "  'category_id': 567,\n",
       "  'id': 5930169957477638665,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 462,\n",
       "  'bbox': [137, 717, 21, 22],\n",
       "  'category_id': 567,\n",
       "  'id': 5930169957476638659,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 42,\n",
       "  'bbox': [384, 410, 6, 7],\n",
       "  'category_id': 567,\n",
       "  'id': 5930169957476638658,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 36,\n",
       "  'bbox': [547, 474, 6, 6],\n",
       "  'category_id': 567,\n",
       "  'id': 5930169957476638657,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 108,\n",
       "  'bbox': [666, 391, 9, 12],\n",
       "  'category_id': 567,\n",
       "  'id': 5930169957476638656,\n",
       "  'image_id': 'a2f7ab86fb274aa0',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_folder = Path(\".\").resolve().parent / \"input\" / \"as_mscoco\" / \"annotations\" \n",
    "annotations_file = output_folder / \"train_overfit.json\"\n",
    "\n",
    "with open(annotations_file.as_posix(), 'r') as h:\n",
    "    annotations = json.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in annotations['annotations']:\n",
    "    bbox = a['bbox']\n",
    "    img_info = [im for im in annotations['images'] if im['id'] == a['image_id']]\n",
    "    assert len(img_info) == 1\n",
    "    img_info = img_info[0]\n",
    "    w = img_info['width']\n",
    "    h = img_info['height']\n",
    "    assert 0 <= bbox[0] <= w, \"Problem with {}, {}\".format(a, img_info)\n",
    "    assert 0 <= bbox[1] <= h, \"Problem with {}, {}\".format(a, img_info)\n",
    "    assert 0 <= bbox[0] + bbox[2] <= w, \"Problem with {}, {}\".format(a, img_info)\n",
    "    assert 0 <= bbox[1] + bbox[3] <= h, \"Problem with {}, {}\".format(a, img_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
