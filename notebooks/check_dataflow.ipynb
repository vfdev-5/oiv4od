{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dataflow:\n",
    "- training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, (Path(\".\").resolve().parent / \"Detectron.pytorch\" / \"lib\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd=!cd ../ && echo ${PWD}/\n",
    "pwd = Path(pwd[0])\n",
    "data_dir = (pwd / \"input\" / \"as_mscoco\")\n",
    "\n",
    "os.environ['DATA_DIR'] = data_dir.as_posix()\n",
    "os.environ['CUSTOM_DATASETS'] = (pwd / \"datasets\" / \"dataset_catalog.py\").as_posix()\n",
    "base_output_path = Path(\"/home/storage_ntfs_1tb\")\n",
    "assert base_output_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.roidb import combined_roidb_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.config import cfg\n",
    "cfg.DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'bbox_targets': array([[391.,   0.,   0.,   0.,   0.],\n",
       "          [391.,   0.,   0.,   0.,   0.],\n",
       "          [391.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2], dtype=int32),\n",
       "   'boxes': array([[  73.,  158.,  147.,  298.],\n",
       "          [ 450.,  202.,  584.,  332.],\n",
       "          [ 684.,    0., 1022.,  423.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([391, 391, 391], dtype=int32),\n",
       "   'gt_overlaps': <3x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '000026e7ee790996',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000026e7ee790996.jpg',\n",
       "   'is_crowd': array([False, False, False]),\n",
       "   'max_classes': array([391, 391, 391]),\n",
       "   'max_overlaps': array([1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([ 10575.,  17685., 143736.], dtype=float32),\n",
       "   'segms': [[], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[ 22.,   0.,   0.,   0.,   0.],\n",
       "          [334.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1], dtype=int32),\n",
       "   'boxes': array([[ 139.,  157.,  576., 1022.],\n",
       "          [  93.,    0.,  255.,  904.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([ 22, 334], dtype=int32),\n",
       "   'gt_overlaps': <2x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 1024,\n",
       "   'id': '000062a39995e348',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000062a39995e348.jpg',\n",
       "   'is_crowd': array([False, False]),\n",
       "   'max_classes': array([ 22, 334]),\n",
       "   'max_overlaps': array([1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([379308., 147515.], dtype=float32),\n",
       "   'segms': [[], []],\n",
       "   'width': 680},\n",
       "  {'bbox_targets': array([[404.,   0.,   0.,   0.,   0.],\n",
       "          [571.,   0.,   0.,   0.,   0.],\n",
       "          [571.,   0.,   0.,   0.,   0.],\n",
       "          [571.,   0.,   0.,   0.,   0.],\n",
       "          [571.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3, 4], dtype=int32),\n",
       "   'boxes': array([[   0.,    0.,  996.,   28.],\n",
       "          [   0.,  219.,  524.,  469.],\n",
       "          [  16.,  204.,  272.,  314.],\n",
       "          [ 493.,  158.,  925.,  332.],\n",
       "          [ 769.,  226., 1022.,  447.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([404, 571, 571, 571, 571], dtype=int32),\n",
       "   'gt_overlaps': <5x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 683,\n",
       "   'id': '0000c64e1253d68f',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0000c64e1253d68f.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False]),\n",
       "   'max_classes': array([404, 571, 571, 571, 571]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([ 28913., 131775.,  28527.,  75775.,  56388.], dtype=float32),\n",
       "   'segms': [[], [], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[ 69.,   0.,   0.,   0.,   0.],\n",
       "          [148.,   0.,   0.,   0.,   0.],\n",
       "          [253.,   0.,   0.,   0.,   0.],\n",
       "          [253.,   0.,   0.,   0.,   0.],\n",
       "          [253.,   0.,   0.,   0.,   0.],\n",
       "          [253.,   0.,   0.,   0.,   0.],\n",
       "          [292.,   0.,   0.,   0.,   0.],\n",
       "          [292.,   0.,   0.,   0.,   0.],\n",
       "          [292.,   0.,   0.,   0.,   0.],\n",
       "          [292.,   0.,   0.,   0.,   0.],\n",
       "          [333.,   0.,   0.,   0.,   0.],\n",
       "          [333.,   0.,   0.,   0.,   0.],\n",
       "          [333.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "          17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
       "         dtype=int32),\n",
       "   'boxes': array([[329., 120., 582., 766.],\n",
       "          [157., 199., 775., 301.],\n",
       "          [138., 204., 235., 278.],\n",
       "          [450., 123., 530., 179.],\n",
       "          [578., 132., 713., 267.],\n",
       "          [710., 198., 868., 337.],\n",
       "          [138., 207., 233., 318.],\n",
       "          [450., 125., 528., 229.],\n",
       "          [595., 133., 668., 243.],\n",
       "          [711., 202., 798., 321.],\n",
       "          [ 34., 207., 287., 766.],\n",
       "          [580., 128., 862., 766.],\n",
       "          [673., 198., 902., 766.],\n",
       "          [ 41., 295., 288., 766.],\n",
       "          [ 44., 204., 897., 766.],\n",
       "          [340., 209., 583., 755.],\n",
       "          [578., 218., 740., 766.],\n",
       "          [685., 302., 899., 766.],\n",
       "          [143., 207., 228., 322.],\n",
       "          [152., 231., 226., 313.],\n",
       "          [450., 121., 530., 228.],\n",
       "          [455., 131., 521., 225.],\n",
       "          [580., 131., 710., 264.],\n",
       "          [600., 152., 661., 243.],\n",
       "          [709., 200., 863., 336.],\n",
       "          [715., 216., 797., 313.],\n",
       "          [ 40., 411.,  92., 618.],\n",
       "          [197., 416., 277., 530.],\n",
       "          [330., 436., 388., 474.],\n",
       "          [575., 483., 583., 523.],\n",
       "          [806., 461., 887., 652.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([ 69, 148, 253, 253, 253, 253, 292, 292, 292, 292, 333, 333, 333,\n",
       "          433, 433, 433, 433, 433, 502, 502, 502, 502, 502, 502, 502, 502,\n",
       "          503, 503, 503, 503, 503], dtype=int32),\n",
       "   'gt_overlaps': <31x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 31 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '000132c20b84269b',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000132c20b84269b.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False]),\n",
       "   'max_classes': array([ 69, 148, 253, 253, 253, 253, 292, 292, 292, 292, 333, 333, 333,\n",
       "          433, 433, 433, 433, 433, 502, 502, 502, 502, 502, 502, 502, 502,\n",
       "          503, 503, 503, 503, 503]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([1.64338e+05, 6.37570e+04, 7.35000e+03, 4.61700e+03, 1.84960e+04,\n",
       "          2.22600e+04, 1.07520e+04, 8.29500e+03, 8.21400e+03, 1.05600e+04,\n",
       "          1.42240e+05, 1.80837e+05, 1.30870e+05, 1.17056e+05, 4.80802e+05,\n",
       "          1.33468e+05, 8.94870e+04, 9.99750e+04, 9.97600e+03, 6.22500e+03,\n",
       "          8.74800e+03, 6.36500e+03, 1.75540e+04, 5.70400e+03, 2.12350e+04,\n",
       "          8.13400e+03, 1.10240e+04, 9.31500e+03, 2.30100e+03, 3.69000e+02,\n",
       "          1.57440e+04], dtype=float32),\n",
       "   'segms': [[],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[292.,   0.,   0.,   0.,   0.],\n",
       "          [391.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [456.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [568.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3, 4, 5], dtype=int32),\n",
       "   'boxes': array([[ 546.,  130.,  761.,  472.],\n",
       "          [   0.,   84., 1022.,  404.],\n",
       "          [ 417.,  394.,  864., 1022.],\n",
       "          [   0.,  305., 1023., 1022.],\n",
       "          [ 586.,  256.,  671.,  328.],\n",
       "          [ 634.,  291.,  653.,  326.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([292, 391, 433, 456, 502, 568], dtype=int32),\n",
       "   'gt_overlaps': <6x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 6 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 1024,\n",
       "   'id': '0002ab0af02e4a77',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002ab0af02e4a77.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False, False]),\n",
       "   'max_classes': array([292, 391, 433, 456, 502, 568]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([7.40880e+04, 3.28383e+05, 2.81792e+05, 7.35232e+05, 6.27800e+03,\n",
       "          7.20000e+02], dtype=float32),\n",
       "   'segms': [[], [], [], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[462.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0], dtype=int32),\n",
       "   'boxes': array([[  15.,    0., 1022.,  639.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([462], dtype=int32),\n",
       "   'gt_overlaps': <1x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '0002cc8afaf1b611',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002cc8afaf1b611.jpg',\n",
       "   'is_crowd': array([False]),\n",
       "   'max_classes': array([462]),\n",
       "   'max_overlaps': array([1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([645120.], dtype=float32),\n",
       "   'segms': [[]],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[38.,  0.,  0.,  0.,  0.],\n",
       "          [69.,  0.,  0.,  0.,  0.],\n",
       "          [69.,  0.,  0.,  0.,  0.],\n",
       "          [69.,  0.,  0.,  0.,  0.],\n",
       "          [69.,  0.,  0.,  0.,  0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3, 4], dtype=int32),\n",
       "   'boxes': array([[  0., 511., 202., 722.],\n",
       "          [231., 152., 301., 331.],\n",
       "          [235.,  21., 298., 202.],\n",
       "          [453.,  26., 523., 226.],\n",
       "          [753.,  11., 856., 294.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([38, 69, 69, 69, 69], dtype=int32),\n",
       "   'gt_overlaps': <5x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '0003d84e0165d630',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0003d84e0165d630.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False]),\n",
       "   'max_classes': array([38, 69, 69, 69, 69]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([43036., 12780., 11648., 14271., 29536.], dtype=float32),\n",
       "   'segms': [[], [], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[334.,   0.,   0.,   0.,   0.],\n",
       "          [422.,   0.,   0.,   0.,   0.],\n",
       "          [422.,   0.,   0.,   0.,   0.],\n",
       "          [422.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3], dtype=int32),\n",
       "   'boxes': array([[   0.,    0., 1022.,  766.],\n",
       "          [ 177.,  151.,  478.,  606.],\n",
       "          [ 460.,  200.,  701.,  422.],\n",
       "          [ 954.,  306., 1022.,  494.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([334, 422, 422, 422], dtype=int32),\n",
       "   'gt_overlaps': <4x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '000411001ff7dd4f',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000411001ff7dd4f.jpg',\n",
       "   'is_crowd': array([False, False, False, False]),\n",
       "   'max_classes': array([334, 422, 422, 422]),\n",
       "   'max_overlaps': array([1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([784641., 137712.,  53966.,  13041.], dtype=float32),\n",
       "   'segms': [[], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[ 69.,   0.,   0.,   0.,   0.],\n",
       "          [ 69.,   0.,   0.,   0.,   0.],\n",
       "          [340.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32),\n",
       "   'boxes': array([[   0.,    0.,  292.,  357.],\n",
       "          [ 452.,    0., 1022.,  535.],\n",
       "          [ 235.,  176.,  736.,  681.],\n",
       "          [ 585.,    0., 1022.,  344.],\n",
       "          [   0.,  339.,  135.,  497.],\n",
       "          [   0.,  477.,  271.,  681.],\n",
       "          [ 183.,  304.,  268.,  444.],\n",
       "          [ 706.,  349.,  919.,  562.],\n",
       "          [ 708.,  220.,  876.,  359.],\n",
       "          [ 781.,  585., 1022.,  681.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([ 69,  69, 340, 433, 500, 500, 500, 500, 500, 500], dtype=int32),\n",
       "   'gt_overlaps': <10x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 683,\n",
       "   'id': '00045d609ca3f4eb',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00045d609ca3f4eb.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False, False, False, False, False,\n",
       "          False]),\n",
       "   'max_classes': array([ 69,  69, 340, 433, 500, 500, 500, 500, 500, 500]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([104894., 306056., 254012., 151110.,  21624.,  55760.,  12126.,\n",
       "           45796.,  23660.,  23474.], dtype=float32),\n",
       "   'segms': [[], [], [], [], [], [], [], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[211.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0], dtype=int32),\n",
       "   'boxes': array([[179., 146., 652., 585.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': False,\n",
       "   'gt_classes': array([211], dtype=int32),\n",
       "   'gt_overlaps': <1x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 681,\n",
       "   'id': '00068d5450f0358b',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00068d5450f0358b.jpg',\n",
       "   'is_crowd': array([False]),\n",
       "   'max_classes': array([211]),\n",
       "   'max_overlaps': array([1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([208560.], dtype=float32),\n",
       "   'segms': [[]],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[391.,   0.,   0.,   0.,   0.],\n",
       "          [391.,   0.,   0.,   0.,   0.],\n",
       "          [391.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2], dtype=int32),\n",
       "   'boxes': array([[876., 158., 950., 298.],\n",
       "          [439., 202., 573., 332.],\n",
       "          [  1.,   0., 339., 423.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([391, 391, 391], dtype=int32),\n",
       "   'gt_overlaps': <3x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 3 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '000026e7ee790996',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000026e7ee790996.jpg',\n",
       "   'is_crowd': array([False, False, False]),\n",
       "   'max_classes': array([391, 391, 391]),\n",
       "   'max_overlaps': array([1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([ 10575.,  17685., 143736.], dtype=float32),\n",
       "   'segms': [[], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[ 22.,   0.,   0.,   0.,   0.],\n",
       "          [334.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1], dtype=int32),\n",
       "   'boxes': array([[ 103.,  157.,  540., 1022.],\n",
       "          [ 424.,    0.,  586.,  904.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([ 22, 334], dtype=int32),\n",
       "   'gt_overlaps': <2x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 2 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 1024,\n",
       "   'id': '000062a39995e348',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000062a39995e348.jpg',\n",
       "   'is_crowd': array([False, False]),\n",
       "   'max_classes': array([ 22, 334]),\n",
       "   'max_overlaps': array([1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([379308., 147515.], dtype=float32),\n",
       "   'segms': [[], []],\n",
       "   'width': 680},\n",
       "  {'bbox_targets': array([[404.,   0.,   0.,   0.,   0.],\n",
       "          [571.,   0.,   0.,   0.,   0.],\n",
       "          [571.,   0.,   0.,   0.,   0.],\n",
       "          [571.,   0.,   0.,   0.,   0.],\n",
       "          [571.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3, 4], dtype=int32),\n",
       "   'boxes': array([[2.700e+01, 0.000e+00, 1.023e+03, 2.800e+01],\n",
       "          [4.990e+02, 2.190e+02, 1.023e+03, 4.690e+02],\n",
       "          [7.510e+02, 2.040e+02, 1.007e+03, 3.140e+02],\n",
       "          [9.800e+01, 1.580e+02, 5.300e+02, 3.320e+02],\n",
       "          [1.000e+00, 2.260e+02, 2.540e+02, 4.470e+02]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([404, 571, 571, 571, 571], dtype=int32),\n",
       "   'gt_overlaps': <5x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 683,\n",
       "   'id': '0000c64e1253d68f',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0000c64e1253d68f.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False]),\n",
       "   'max_classes': array([404, 571, 571, 571, 571]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([ 28913., 131775.,  28527.,  75775.,  56388.], dtype=float32),\n",
       "   'segms': [[], [], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[ 69.,   0.,   0.,   0.,   0.],\n",
       "          [148.,   0.,   0.,   0.,   0.],\n",
       "          [253.,   0.,   0.,   0.,   0.],\n",
       "          [253.,   0.,   0.,   0.,   0.],\n",
       "          [253.,   0.,   0.,   0.,   0.],\n",
       "          [253.,   0.,   0.,   0.,   0.],\n",
       "          [292.,   0.,   0.,   0.,   0.],\n",
       "          [292.,   0.,   0.,   0.,   0.],\n",
       "          [292.,   0.,   0.,   0.,   0.],\n",
       "          [292.,   0.,   0.,   0.,   0.],\n",
       "          [333.,   0.,   0.,   0.,   0.],\n",
       "          [333.,   0.,   0.,   0.,   0.],\n",
       "          [333.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.],\n",
       "          [503.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "          17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
       "         dtype=int32),\n",
       "   'boxes': array([[441., 120., 694., 766.],\n",
       "          [248., 199., 866., 301.],\n",
       "          [788., 204., 885., 278.],\n",
       "          [493., 123., 573., 179.],\n",
       "          [310., 132., 445., 267.],\n",
       "          [155., 198., 313., 337.],\n",
       "          [790., 207., 885., 318.],\n",
       "          [495., 125., 573., 229.],\n",
       "          [355., 133., 428., 243.],\n",
       "          [225., 202., 312., 321.],\n",
       "          [736., 207., 989., 766.],\n",
       "          [161., 128., 443., 766.],\n",
       "          [121., 198., 350., 766.],\n",
       "          [735., 295., 982., 766.],\n",
       "          [126., 204., 979., 766.],\n",
       "          [440., 209., 683., 755.],\n",
       "          [283., 218., 445., 766.],\n",
       "          [124., 302., 338., 766.],\n",
       "          [795., 207., 880., 322.],\n",
       "          [797., 231., 871., 313.],\n",
       "          [493., 121., 573., 228.],\n",
       "          [502., 131., 568., 225.],\n",
       "          [313., 131., 443., 264.],\n",
       "          [362., 152., 423., 243.],\n",
       "          [160., 200., 314., 336.],\n",
       "          [226., 216., 308., 313.],\n",
       "          [931., 411., 983., 618.],\n",
       "          [746., 416., 826., 530.],\n",
       "          [635., 436., 693., 474.],\n",
       "          [440., 483., 448., 523.],\n",
       "          [136., 461., 217., 652.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([ 69, 148, 253, 253, 253, 253, 292, 292, 292, 292, 333, 333, 333,\n",
       "          433, 433, 433, 433, 433, 502, 502, 502, 502, 502, 502, 502, 502,\n",
       "          503, 503, 503, 503, 503], dtype=int32),\n",
       "   'gt_overlaps': <31x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 31 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '000132c20b84269b',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000132c20b84269b.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False]),\n",
       "   'max_classes': array([ 69, 148, 253, 253, 253, 253, 292, 292, 292, 292, 333, 333, 333,\n",
       "          433, 433, 433, 433, 433, 502, 502, 502, 502, 502, 502, 502, 502,\n",
       "          503, 503, 503, 503, 503]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([1.64338e+05, 6.37570e+04, 7.35000e+03, 4.61700e+03, 1.84960e+04,\n",
       "          2.22600e+04, 1.07520e+04, 8.29500e+03, 8.21400e+03, 1.05600e+04,\n",
       "          1.42240e+05, 1.80837e+05, 1.30870e+05, 1.17056e+05, 4.80802e+05,\n",
       "          1.33468e+05, 8.94870e+04, 9.99750e+04, 9.97600e+03, 6.22500e+03,\n",
       "          8.74800e+03, 6.36500e+03, 1.75540e+04, 5.70400e+03, 2.12350e+04,\n",
       "          8.13400e+03, 1.10240e+04, 9.31500e+03, 2.30100e+03, 3.69000e+02,\n",
       "          1.57440e+04], dtype=float32),\n",
       "   'segms': [[],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[292.,   0.,   0.,   0.,   0.],\n",
       "          [391.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [456.,   0.,   0.,   0.,   0.],\n",
       "          [502.,   0.,   0.,   0.,   0.],\n",
       "          [568.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3, 4, 5], dtype=int32),\n",
       "   'boxes': array([[2.620e+02, 1.300e+02, 4.770e+02, 4.720e+02],\n",
       "          [1.000e+00, 8.400e+01, 1.023e+03, 4.040e+02],\n",
       "          [1.590e+02, 3.940e+02, 6.060e+02, 1.022e+03],\n",
       "          [0.000e+00, 3.050e+02, 1.023e+03, 1.022e+03],\n",
       "          [3.520e+02, 2.560e+02, 4.370e+02, 3.280e+02],\n",
       "          [3.700e+02, 2.910e+02, 3.890e+02, 3.260e+02]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([292, 391, 433, 456, 502, 568], dtype=int32),\n",
       "   'gt_overlaps': <6x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 6 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 1024,\n",
       "   'id': '0002ab0af02e4a77',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002ab0af02e4a77.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False, False]),\n",
       "   'max_classes': array([292, 391, 433, 456, 502, 568]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([7.40880e+04, 3.28383e+05, 2.81792e+05, 7.35232e+05, 6.27800e+03,\n",
       "          7.20000e+02], dtype=float32),\n",
       "   'segms': [[], [], [], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[462.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0], dtype=int32),\n",
       "   'boxes': array([[1.000e+00, 0.000e+00, 1.008e+03, 6.390e+02]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([462], dtype=int32),\n",
       "   'gt_overlaps': <1x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '0002cc8afaf1b611',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002cc8afaf1b611.jpg',\n",
       "   'is_crowd': array([False]),\n",
       "   'max_classes': array([462]),\n",
       "   'max_overlaps': array([1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([645120.], dtype=float32),\n",
       "   'segms': [[]],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[38.,  0.,  0.,  0.,  0.],\n",
       "          [69.,  0.,  0.,  0.,  0.],\n",
       "          [69.,  0.,  0.,  0.,  0.],\n",
       "          [69.,  0.,  0.,  0.,  0.],\n",
       "          [69.,  0.,  0.,  0.,  0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3, 4], dtype=int32),\n",
       "   'boxes': array([[ 821.,  511., 1023.,  722.],\n",
       "          [ 722.,  152.,  792.,  331.],\n",
       "          [ 725.,   21.,  788.,  202.],\n",
       "          [ 500.,   26.,  570.,  226.],\n",
       "          [ 167.,   11.,  270.,  294.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([38, 69, 69, 69, 69], dtype=int32),\n",
       "   'gt_overlaps': <5x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 5 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '0003d84e0165d630',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0003d84e0165d630.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False]),\n",
       "   'max_classes': array([38, 69, 69, 69, 69]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([43036., 12780., 11648., 14271., 29536.], dtype=float32),\n",
       "   'segms': [[], [], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[334.,   0.,   0.,   0.,   0.],\n",
       "          [422.,   0.,   0.,   0.,   0.],\n",
       "          [422.,   0.,   0.,   0.,   0.],\n",
       "          [422.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3], dtype=int32),\n",
       "   'boxes': array([[1.000e+00, 0.000e+00, 1.023e+03, 7.660e+02],\n",
       "          [5.450e+02, 1.510e+02, 8.460e+02, 6.060e+02],\n",
       "          [3.220e+02, 2.000e+02, 5.630e+02, 4.220e+02],\n",
       "          [1.000e+00, 3.060e+02, 6.900e+01, 4.940e+02]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([334, 422, 422, 422], dtype=int32),\n",
       "   'gt_overlaps': <4x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 4 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 768,\n",
       "   'id': '000411001ff7dd4f',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000411001ff7dd4f.jpg',\n",
       "   'is_crowd': array([False, False, False, False]),\n",
       "   'max_classes': array([334, 422, 422, 422]),\n",
       "   'max_overlaps': array([1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([784641., 137712.,  53966.,  13041.], dtype=float32),\n",
       "   'segms': [[], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[ 69.,   0.,   0.,   0.,   0.],\n",
       "          [ 69.,   0.,   0.,   0.,   0.],\n",
       "          [340.,   0.,   0.,   0.,   0.],\n",
       "          [433.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.],\n",
       "          [500.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32),\n",
       "   'boxes': array([[7.310e+02, 0.000e+00, 1.023e+03, 3.570e+02],\n",
       "          [1.000e+00, 0.000e+00, 5.710e+02, 5.350e+02],\n",
       "          [2.870e+02, 1.760e+02, 7.880e+02, 6.810e+02],\n",
       "          [1.000e+00, 0.000e+00, 4.380e+02, 3.440e+02],\n",
       "          [8.880e+02, 3.390e+02, 1.023e+03, 4.970e+02],\n",
       "          [7.520e+02, 4.770e+02, 1.023e+03, 6.810e+02],\n",
       "          [7.550e+02, 3.040e+02, 8.400e+02, 4.440e+02],\n",
       "          [1.040e+02, 3.490e+02, 3.170e+02, 5.620e+02],\n",
       "          [1.470e+02, 2.200e+02, 3.150e+02, 3.590e+02],\n",
       "          [1.000e+00, 5.850e+02, 2.420e+02, 6.810e+02]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([ 69,  69, 340, 433, 500, 500, 500, 500, 500, 500], dtype=int32),\n",
       "   'gt_overlaps': <10x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 10 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 683,\n",
       "   'id': '00045d609ca3f4eb',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00045d609ca3f4eb.jpg',\n",
       "   'is_crowd': array([False, False, False, False, False, False, False, False, False,\n",
       "          False]),\n",
       "   'max_classes': array([ 69,  69, 340, 433, 500, 500, 500, 500, 500, 500]),\n",
       "   'max_overlaps': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([104894., 306056., 254012., 151110.,  21624.,  55760.,  12126.,\n",
       "           45796.,  23660.,  23474.], dtype=float32),\n",
       "   'segms': [[], [], [], [], [], [], [], [], [], []],\n",
       "   'width': 1024},\n",
       "  {'bbox_targets': array([[211.,   0.,   0.,   0.,   0.]], dtype=float32),\n",
       "   'box_to_gt_ind_map': array([0], dtype=int32),\n",
       "   'boxes': array([[371., 146., 844., 585.]], dtype=float32),\n",
       "   'dataset': <datasets.json_dataset.JsonDataset at 0x7fe6e964ddd8>,\n",
       "   'flipped': True,\n",
       "   'gt_classes': array([211], dtype=int32),\n",
       "   'gt_overlaps': <1x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "   \twith 1 stored elements in Compressed Sparse Row format>,\n",
       "   'has_visible_keypoints': False,\n",
       "   'height': 681,\n",
       "   'id': '00068d5450f0358b',\n",
       "   'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00068d5450f0358b.jpg',\n",
       "   'is_crowd': array([False]),\n",
       "   'max_classes': array([211]),\n",
       "   'max_overlaps': array([1.], dtype=float32),\n",
       "   'need_crop': False,\n",
       "   'seg_areas': array([208560.], dtype=float32),\n",
       "   'segms': [[]],\n",
       "   'width': 1024}],\n",
       " array([0.6640625 , 0.6640625 , 1.        , 1.        , 1.33333333,\n",
       "        1.33333333, 1.33333333, 1.33333333, 1.33333333, 1.33333333,\n",
       "        1.33333333, 1.33333333, 1.33333333, 1.33333333, 1.49926794,\n",
       "        1.49926794, 1.49926794, 1.49926794, 1.50367107, 1.50367107]),\n",
       " array([ 1, 11, 14,  4,  0, 15, 13, 10, 17,  7,  6,  5,  3, 16, 18, 12,  2,\n",
       "         8,  9, 19]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_roidb_for_training((\"open_images_v4_train_overfit\", ), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = (\"open_images_v4_train_overfit\", )\n",
    "proposal_files = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from past.builtins import basestring\n",
    "\n",
    "from core.config import cfg\n",
    "from datasets.json_dataset import JsonDataset\n",
    "import utils.boxes as box_utils\n",
    "import utils.keypoints as keypoint_utils\n",
    "import utils.segms as segm_utils\n",
    "\n",
    "\n",
    "def get_roidb(dataset_name, proposal_file):\n",
    "    ds = JsonDataset(dataset_name)\n",
    "    roidb = ds.get_roidb(\n",
    "        gt=True,\n",
    "        proposal_file=proposal_file,\n",
    "        crowd_filter_thresh=cfg.TRAIN.CROWD_FILTER_THRESH\n",
    "    )\n",
    "    return roidb\n",
    "\n",
    "if isinstance(dataset_names, basestring):\n",
    "    dataset_names = (dataset_names, )\n",
    "if isinstance(proposal_files, basestring):\n",
    "    proposal_files = (proposal_files, )\n",
    "if len(proposal_files) == 0:\n",
    "    proposal_files = (None, ) * len(dataset_names)\n",
    "assert len(dataset_names) == len(proposal_files)\n",
    "\n",
    "# roidbs = [get_roidb(*args) for args in zip(dataset_names, proposal_files)]\n",
    "\n",
    "# roidb = roidbs[0]\n",
    "# for r in roidbs[1:]:\n",
    "#     roidb.extend(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "ds = JsonDataset(\"open_images_v4_train_overfit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000026e7ee790996',\n",
       " '000062a39995e348',\n",
       " '00045d609ca3f4eb',\n",
       " '000411001ff7dd4f',\n",
       " '0002ab0af02e4a77',\n",
       " '0000c64e1253d68f',\n",
       " '0002cc8afaf1b611',\n",
       " '00068d5450f0358b',\n",
       " '000132c20b84269b',\n",
       " '0003d84e0165d630']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids = ds.COCO.getImgIds()\n",
    "image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "roidb = ds.COCO.loadImgs(image_ids)\n",
    "\n",
    "for entry in roidb:\n",
    "    ds._prep_roidb_entry(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in roidb:\n",
    "    ds._add_gt_annotations(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-9042526684225884363,\n",
       " 2535230105118440843,\n",
       " 5498232473603185199,\n",
       " -5135691115185693684,\n",
       " -8324176295614756880,\n",
       " 1488437518732544179]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.COCO.getAnnIds(imgIds=['0002ab0af02e4a77', ], iscrowd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000026e7ee790996': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000026e7ee790996',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000026e7ee790996.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " '000062a39995e348': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 1024,\n",
       "  'id': '000062a39995e348',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000062a39995e348.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 680},\n",
       " '0000c64e1253d68f': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 683,\n",
       "  'id': '0000c64e1253d68f',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0000c64e1253d68f.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " '000132c20b84269b': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000132c20b84269b',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000132c20b84269b.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " '0002ab0af02e4a77': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 1024,\n",
       "  'id': '0002ab0af02e4a77',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002ab0af02e4a77.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " '0002cc8afaf1b611': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '0002cc8afaf1b611',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002cc8afaf1b611.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " '0003d84e0165d630': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '0003d84e0165d630',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0003d84e0165d630.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " '000411001ff7dd4f': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000411001ff7dd4f',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000411001ff7dd4f.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " '00045d609ca3f4eb': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 683,\n",
       "  'id': '00045d609ca3f4eb',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00045d609ca3f4eb.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " '00068d5450f0358b': {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef450f0>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 681,\n",
       "  'id': '00068d5450f0358b',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00068d5450f0358b.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.COCO.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-9042526684225884363: {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 74088,\n",
       "  'bbox': [546, 130, 216, 343],\n",
       "  'category_id': 291,\n",
       "  'id': -9042526684225884363,\n",
       "  'image_id': '0002ab0af02e4a77',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -9039809453063774053: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 10560,\n",
       "  'bbox': [711, 202, 88, 120],\n",
       "  'category_id': 291,\n",
       "  'id': -9039809453063774053,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -8934795629177218895: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 8295,\n",
       "  'bbox': [450, 125, 79, 105],\n",
       "  'category_id': 291,\n",
       "  'id': -8934795629177218895,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -8641439857406648307: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 4617,\n",
       "  'bbox': [450, 123, 81, 57],\n",
       "  'category_id': 252,\n",
       "  'id': -8641439857406648307,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -8392246040157811323: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 17554,\n",
       "  'bbox': [580, 131, 131, 134],\n",
       "  'category_id': 501,\n",
       "  'id': -8392246040157811323,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -8367790989027541096: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 89487,\n",
       "  'bbox': [578, 218, 163, 549],\n",
       "  'category_id': 432,\n",
       "  'id': -8367790989027541096,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -8324176295614756880: {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 6278,\n",
       "  'bbox': [586, 256, 86, 73],\n",
       "  'category_id': 501,\n",
       "  'id': -8324176295614756880,\n",
       "  'image_id': '0002ab0af02e4a77',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -8100503171567255231: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 29536,\n",
       "  'bbox': [753, 11, 104, 284],\n",
       "  'category_id': 68,\n",
       "  'id': -8100503171567255231,\n",
       "  'image_id': '0003d84e0165d630',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -8095394593219702017: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 254012,\n",
       "  'bbox': [235, 176, 502, 506],\n",
       "  'category_id': 339,\n",
       "  'id': -8095394593219702017,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -7946157634651691374: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 9315,\n",
       "  'bbox': [197, 416, 81, 115],\n",
       "  'category_id': 502,\n",
       "  'id': -7946157634651691374,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -7568064923656450838: {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 12126,\n",
       "  'bbox': [183, 304, 86, 141],\n",
       "  'category_id': 499,\n",
       "  'id': -7568064923656450838,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -7452788367064539052: {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 55760,\n",
       "  'bbox': [0, 477, 272, 205],\n",
       "  'category_id': 499,\n",
       "  'id': -7452788367064539052,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -7157369055407135860: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 18496,\n",
       "  'bbox': [578, 132, 136, 136],\n",
       "  'category_id': 252,\n",
       "  'id': -7157369055407135860,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -6805225864775041882: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 480802,\n",
       "  'bbox': [44, 204, 854, 563],\n",
       "  'category_id': 432,\n",
       "  'id': -6805225864775041882,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -6366663860096278812: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 28527,\n",
       "  'bbox': [16, 204, 257, 111],\n",
       "  'category_id': 570,\n",
       "  'id': -6366663860096278812,\n",
       "  'image_id': '0000c64e1253d68f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -6049596336909712660: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 142240,\n",
       "  'bbox': [34, 207, 254, 560],\n",
       "  'category_id': 332,\n",
       "  'id': -6049596336909712660,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -5623316178241688922: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 2301,\n",
       "  'bbox': [330, 436, 59, 39],\n",
       "  'category_id': 502,\n",
       "  'id': -5623316178241688922,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -5195496846139448798: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 7350,\n",
       "  'bbox': [138, 204, 98, 75],\n",
       "  'category_id': 252,\n",
       "  'id': -5195496846139448798,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -5135691115185693684: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 735232,\n",
       "  'bbox': [0, 305, 1024, 718],\n",
       "  'category_id': 455,\n",
       "  'id': -5135691115185693684,\n",
       "  'image_id': '0002ab0af02e4a77',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -5108073702562241755: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 75775,\n",
       "  'bbox': [493, 158, 433, 175],\n",
       "  'category_id': 570,\n",
       "  'id': -5108073702562241755,\n",
       "  'image_id': '0000c64e1253d68f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -4880870107667655760: {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 21624,\n",
       "  'bbox': [0, 339, 136, 159],\n",
       "  'category_id': 499,\n",
       "  'id': -4880870107667655760,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -3859837759609560393: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 56388,\n",
       "  'bbox': [769, 226, 254, 222],\n",
       "  'category_id': 570,\n",
       "  'id': -3859837759609560393,\n",
       "  'image_id': '0000c64e1253d68f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -3759115178929056412: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 369,\n",
       "  'bbox': [575, 483, 9, 41],\n",
       "  'category_id': 502,\n",
       "  'id': -3759115178929056412,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -3669130959035134006: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 23474,\n",
       "  'bbox': [781, 585, 242, 97],\n",
       "  'category_id': 499,\n",
       "  'id': -3669130959035134006,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -3369976907502542096: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 43036,\n",
       "  'bbox': [0, 511, 203, 212],\n",
       "  'category_id': 37,\n",
       "  'id': -3369976907502542096,\n",
       "  'image_id': '0003d84e0165d630',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -3330175118212761692: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 137712,\n",
       "  'bbox': [177, 151, 302, 456],\n",
       "  'category_id': 421,\n",
       "  'id': -3330175118212761692,\n",
       "  'image_id': '000411001ff7dd4f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -3291597145489384563: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 15744,\n",
       "  'bbox': [806, 461, 82, 192],\n",
       "  'category_id': 502,\n",
       "  'id': -3291597145489384563,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -2569454310398831601: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 306056,\n",
       "  'bbox': [452, 0, 571, 536],\n",
       "  'category_id': 68,\n",
       "  'id': -2569454310398831601,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -2226426916998938065: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 6365,\n",
       "  'bbox': [455, 131, 67, 95],\n",
       "  'category_id': 501,\n",
       "  'id': -2226426916998938065,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -2038826039697720024: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 10752,\n",
       "  'bbox': [138, 207, 96, 112],\n",
       "  'category_id': 291,\n",
       "  'id': -2038826039697720024,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -1125511474194030486: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 143736,\n",
       "  'bbox': [684, 0, 339, 424],\n",
       "  'category_id': 390,\n",
       "  'id': -1125511474194030486,\n",
       "  'image_id': '000026e7ee790996',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -827247603965349142: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 11648,\n",
       "  'bbox': [235, 21, 64, 182],\n",
       "  'category_id': 68,\n",
       "  'id': -827247603965349142,\n",
       "  'image_id': '0003d84e0165d630',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -346073449823435039: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 5704,\n",
       "  'bbox': [600, 152, 62, 92],\n",
       "  'category_id': 501,\n",
       "  'id': -346073449823435039,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -139480886076406290: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 147515,\n",
       "  'bbox': [93, 0, 163, 905],\n",
       "  'category_id': 333,\n",
       "  'id': -139480886076406290,\n",
       "  'image_id': '000062a39995e348',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " -67661707244142464: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 131775,\n",
       "  'bbox': [0, 219, 525, 251],\n",
       "  'category_id': 570,\n",
       "  'id': -67661707244142464,\n",
       "  'image_id': '0000c64e1253d68f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 385909302438221915: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 8134,\n",
       "  'bbox': [715, 216, 83, 98],\n",
       "  'category_id': 501,\n",
       "  'id': 385909302438221915,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 920286457323564718: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 151110,\n",
       "  'bbox': [585, 0, 438, 345],\n",
       "  'category_id': 432,\n",
       "  'id': 920286457323564718,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 972244729320405638: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 130870,\n",
       "  'bbox': [673, 198, 230, 569],\n",
       "  'category_id': 332,\n",
       "  'id': 972244729320405638,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 1057365085034574689: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 8214,\n",
       "  'bbox': [595, 133, 74, 111],\n",
       "  'category_id': 291,\n",
       "  'id': 1057365085034574689,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 1088628681755236284: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 28913,\n",
       "  'bbox': [0, 0, 997, 29],\n",
       "  'category_id': 403,\n",
       "  'id': 1088628681755236284,\n",
       "  'image_id': '0000c64e1253d68f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 1488437518732544179: {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 720,\n",
       "  'bbox': [634, 291, 20, 36],\n",
       "  'category_id': 567,\n",
       "  'id': 1488437518732544179,\n",
       "  'image_id': '0002ab0af02e4a77',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 1553134930807175409: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 10575,\n",
       "  'bbox': [73, 158, 75, 141],\n",
       "  'category_id': 390,\n",
       "  'id': 1553134930807175409,\n",
       "  'image_id': '000026e7ee790996',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 1605313359648327121: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 117056,\n",
       "  'bbox': [41, 295, 248, 472],\n",
       "  'category_id': 432,\n",
       "  'id': 1605313359648327121,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 1930731826159863252: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 8748,\n",
       "  'bbox': [450, 121, 81, 108],\n",
       "  'category_id': 501,\n",
       "  'id': 1930731826159863252,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 1968742797309447782: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 784641,\n",
       "  'bbox': [0, 0, 1023, 767],\n",
       "  'category_id': 333,\n",
       "  'id': 1968742797309447782,\n",
       "  'image_id': '000411001ff7dd4f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 2103524210677898640: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 164338,\n",
       "  'bbox': [329, 120, 254, 647],\n",
       "  'category_id': 68,\n",
       "  'id': 2103524210677898640,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 2535230105118440843: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 328383,\n",
       "  'bbox': [0, 84, 1023, 321],\n",
       "  'category_id': 390,\n",
       "  'id': 2535230105118440843,\n",
       "  'image_id': '0002ab0af02e4a77',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 2906677071031751640: {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 45796,\n",
       "  'bbox': [706, 349, 214, 214],\n",
       "  'category_id': 499,\n",
       "  'id': 2906677071031751640,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 3725297933057883595: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 13041,\n",
       "  'bbox': [954, 306, 69, 189],\n",
       "  'category_id': 421,\n",
       "  'id': 3725297933057883595,\n",
       "  'image_id': '000411001ff7dd4f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 3945416055936265300: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 22260,\n",
       "  'bbox': [710, 198, 159, 140],\n",
       "  'category_id': 252,\n",
       "  'id': 3945416055936265300,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 4189645461358665255: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 6225,\n",
       "  'bbox': [152, 231, 75, 83],\n",
       "  'category_id': 501,\n",
       "  'id': 4189645461358665255,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 4456057496590039227: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 133468,\n",
       "  'bbox': [340, 209, 244, 547],\n",
       "  'category_id': 432,\n",
       "  'id': 4456057496590039227,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 4456187268741061440: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 12780,\n",
       "  'bbox': [231, 152, 71, 180],\n",
       "  'category_id': 68,\n",
       "  'id': 4456187268741061440,\n",
       "  'image_id': '0003d84e0165d630',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 4659227992710292157: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 63757,\n",
       "  'bbox': [157, 199, 619, 103],\n",
       "  'category_id': 147,\n",
       "  'id': 4659227992710292157,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 4823534548418808863: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 11024,\n",
       "  'bbox': [40, 411, 53, 208],\n",
       "  'category_id': 502,\n",
       "  'id': 4823534548418808863,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 5498232473603185199: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 281792,\n",
       "  'bbox': [417, 394, 448, 629],\n",
       "  'category_id': 432,\n",
       "  'id': 5498232473603185199,\n",
       "  'image_id': '0002ab0af02e4a77',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 6412292426659854025: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 9976,\n",
       "  'bbox': [143, 207, 86, 116],\n",
       "  'category_id': 501,\n",
       "  'id': 6412292426659854025,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 6962733274537133371: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 180837,\n",
       "  'bbox': [580, 128, 283, 639],\n",
       "  'category_id': 332,\n",
       "  'id': 6962733274537133371,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 6985892291194040583: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 99975,\n",
       "  'bbox': [685, 302, 215, 465],\n",
       "  'category_id': 432,\n",
       "  'id': 6985892291194040583,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 7015463969217784018: {'IsDepiction': 1,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 23660,\n",
       "  'bbox': [708, 220, 169, 140],\n",
       "  'category_id': 499,\n",
       "  'id': 7015463969217784018,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 7041668380610162233: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 379308,\n",
       "  'bbox': [139, 157, 438, 866],\n",
       "  'category_id': 21,\n",
       "  'id': 7041668380610162233,\n",
       "  'image_id': '000062a39995e348',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 7438749310974354874: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 1,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 104894,\n",
       "  'bbox': [0, 0, 293, 358],\n",
       "  'category_id': 68,\n",
       "  'id': 7438749310974354874,\n",
       "  'image_id': '00045d609ca3f4eb',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 8028685956316803149: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 21235,\n",
       "  'bbox': [709, 200, 155, 137],\n",
       "  'category_id': 501,\n",
       "  'id': 8028685956316803149,\n",
       "  'image_id': '000132c20b84269b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 8145055732570131015: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 17685,\n",
       "  'bbox': [450, 202, 135, 131],\n",
       "  'category_id': 390,\n",
       "  'id': 8145055732570131015,\n",
       "  'image_id': '000026e7ee790996',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 8693523635497879132: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 53966,\n",
       "  'bbox': [460, 200, 242, 223],\n",
       "  'category_id': 421,\n",
       "  'id': 8693523635497879132,\n",
       "  'image_id': '000411001ff7dd4f',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 8814227605525342591: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 1,\n",
       "  'area': 645120,\n",
       "  'bbox': [15, 0, 1008, 640],\n",
       "  'category_id': 461,\n",
       "  'id': 8814227605525342591,\n",
       "  'image_id': '0002cc8afaf1b611',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 9059591747546529351: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 14271,\n",
       "  'bbox': [453, 26, 71, 201],\n",
       "  'category_id': 68,\n",
       "  'id': 9059591747546529351,\n",
       "  'image_id': '0003d84e0165d630',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []},\n",
       " 9106529487768467611: {'IsDepiction': 0,\n",
       "  'IsInside': 0,\n",
       "  'IsOccluded': 0,\n",
       "  'IsTruncated': 0,\n",
       "  'area': 208560,\n",
       "  'bbox': [179, 146, 474, 440],\n",
       "  'category_id': 210,\n",
       "  'id': 9106529487768467611,\n",
       "  'image_id': '00068d5450f0358b',\n",
       "  'iscrowd': 0,\n",
       "  'segmentation': []}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.COCO.anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '0003d84e0165d630',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0003d84e0165d630.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000026e7ee790996',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000026e7ee790996.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 683,\n",
       "  'id': '00045d609ca3f4eb',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00045d609ca3f4eb.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 1024,\n",
       "  'id': '000062a39995e348',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000062a39995e348.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 680},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 681,\n",
       "  'id': '00068d5450f0358b',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00068d5450f0358b.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000132c20b84269b',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000132c20b84269b.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '0002cc8afaf1b611',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002cc8afaf1b611.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 683,\n",
       "  'id': '0000c64e1253d68f',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0000c64e1253d68f.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000411001ff7dd4f',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000411001ff7dd4f.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3ef94240>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 1024,\n",
       "  'id': '0002ab0af02e4a77',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002ab0af02e4a77.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roidb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "roidb = ds.get_roidb(\n",
    "    gt=True,\n",
    "    proposal_file=None,\n",
    "    crowd_filter_thresh=cfg.TRAIN.CROWD_FILTER_THRESH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000026e7ee790996',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000026e7ee790996.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 1024,\n",
       "  'id': '000062a39995e348',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000062a39995e348.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 680},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 683,\n",
       "  'id': '0000c64e1253d68f',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0000c64e1253d68f.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000132c20b84269b',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000132c20b84269b.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 1024,\n",
       "  'id': '0002ab0af02e4a77',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002ab0af02e4a77.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '0002cc8afaf1b611',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002cc8afaf1b611.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '0003d84e0165d630',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0003d84e0165d630.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 768,\n",
       "  'id': '000411001ff7dd4f',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000411001ff7dd4f.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 683,\n",
       "  'id': '00045d609ca3f4eb',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00045d609ca3f4eb.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024},\n",
       " {'box_to_gt_ind_map': array([], dtype=int32),\n",
       "  'boxes': array([], shape=(0, 4), dtype=float32),\n",
       "  'dataset': <datasets.json_dataset.JsonDataset at 0x7fce3f9967b8>,\n",
       "  'flipped': False,\n",
       "  'gt_classes': array([], dtype=int32),\n",
       "  'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
       "  \twith 0 stored elements in Compressed Sparse Row format>,\n",
       "  'has_visible_keypoints': False,\n",
       "  'height': 681,\n",
       "  'id': '00068d5450f0358b',\n",
       "  'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00068d5450f0358b.jpg',\n",
       "  'is_crowd': array([], dtype=bool),\n",
       "  'max_classes': array([], dtype=int64),\n",
       "  'max_overlaps': array([], dtype=float32),\n",
       "  'seg_areas': array([], dtype=float32),\n",
       "  'segms': [],\n",
       "  'width': 1024}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roidb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roidb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000026e7ee790996.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '000026e7ee790996', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 768}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000062a39995e348.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '000062a39995e348', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 680, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 1024}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0000c64e1253d68f.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '0000c64e1253d68f', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 683}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000132c20b84269b.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '000132c20b84269b', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 768}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002ab0af02e4a77.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '0002ab0af02e4a77', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 1024}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002cc8afaf1b611.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '0002cc8afaf1b611', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 768}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0003d84e0165d630.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '0003d84e0165d630', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 768}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000411001ff7dd4f.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '000411001ff7dd4f', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 768}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00045d609ca3f4eb.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '00045d609ca3f4eb', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 683}\n",
      "{'gt_classes': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00068d5450f0358b.jpg', 'segms': [], 'boxes': array([], shape=(0, 4), dtype=float32), 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fce3f6edf60>, 'box_to_gt_ind_map': array([], dtype=int32), 'id': '00068d5450f0358b', 'has_visible_keypoints': False, 'max_overlaps': array([], dtype=float32), 'is_crowd': array([], dtype=bool), 'seg_areas': array([], dtype=float32), 'width': 1024, 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'flipped': False, 'max_classes': array([], dtype=int64), 'height': 681}\n"
     ]
    }
   ],
   "source": [
    "for r in roidb:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../Detectron.pytorch/data/cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation_file': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/annotations/train_overfit.json',\n",
       " 'image_directory': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.dataset_catalog import DATASETS\n",
    "\n",
    "DATASETS[dataset_names[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(DATASETS[dataset_names[0]]['annotation_file'], 'r') as h:\n",
    "    data = json.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 768, 'id': '000026e7ee790996', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000026e7ee790996.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 680, 'height': 1024, 'id': '000062a39995e348', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000062a39995e348.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 683, 'id': '0000c64e1253d68f', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0000c64e1253d68f.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 768, 'id': '000132c20b84269b', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000132c20b84269b.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 1024, 'id': '0002ab0af02e4a77', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002ab0af02e4a77.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 768, 'id': '0002cc8afaf1b611', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0002cc8afaf1b611.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 768, 'id': '0003d84e0165d630', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/0003d84e0165d630.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 768, 'id': '000411001ff7dd4f', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/000411001ff7dd4f.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 683, 'id': '00045d609ca3f4eb', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00045d609ca3f4eb.jpg'}\n",
      "{'gt_classes': array([], dtype=int32), 'max_overlaps': array([], dtype=float32), 'boxes': array([], shape=(0, 4), dtype=float32), 'seg_areas': array([], dtype=float32), 'gt_overlaps': <0x602 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>, 'has_visible_keypoints': False, 'width': 1024, 'height': 681, 'id': '00068d5450f0358b', 'max_classes': array([], dtype=int64), 'flipped': False, 'dataset': <datasets.json_dataset.JsonDataset object at 0x7fd298e50208>, 'segms': [], 'is_crowd': array([], dtype=bool), 'box_to_gt_ind_map': array([], dtype=int32), 'image': '/home/working_directory/ml/kaggle/OpenImagesObjectDetection/input/as_mscoco/train_overfit/00068d5450f0358b.jpg'}\n"
     ]
    }
   ],
   "source": [
    "for r in roidb:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import finalize_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
